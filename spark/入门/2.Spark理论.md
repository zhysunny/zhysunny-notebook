# Spark理论

### Spark验证

1.本地模式(local)

2.集群模式Spark Standalone，
spark本身的集群（Master Worker）

3.集群模式Spark on Yarn上的yarn-cluster模式

### Spark Standalone  VS.  Spark on Yarn

**Spark Standalone** 独立模式，类似于MR1.0模式，完全由内部实现容错性和资源管理

**Spark on Yarn** 让spark运行在一个通用的资源管理系统之上，这样可以与其他计算框架共享资源

### Yarn Client  VS. Yarn Cluster
**Yarn Client**：适用于交互与调试

* Driver在任务提交上执行
* AM只负责向RM申请executor需要的资源
* 基于yarn时，spark-shell和pyspark必须使用yarn-client模式

**Yarn Cluster**：适用于生产环境

Yarn Client  VS. Yarn Cluster区别本质是AM进程的区别，cluster模式下，driver运行在AM中，负责向Yarn申请资源，并监督作业运行状况，当用户提交完作业后，就关掉Client，作业会继续在yarn上运行，然而cluster模式不适合交互类型的作业。而Client模式，AM仅向yarn请求executor，client会和请求的container通信来调度任务，即client不能离开

### Spark和Hadoop作业之间的区别
**Hadoop中**：

* 一个MapReduce程序就是一个job，而一个job里面可以有一个或者多个Task，Task又可以区分为Map Task和Reduce Task

* MapReduce中每个Task分别在自己的进程中运行，当该Task运行完，进程就结束了

**Spark中**：

* 同样有job的概念，但这里的job和MR中的job不一样
* 一个Application和一个SparkContext相关联，每个Application中有一个或者多个job，可以并行或者串行运行job
* Spark中一个Action可以触发一个job运行
* 在job里面包含多个stage，stage以shuffle进行划分，stage中包含多个Task，多个Task构建Task Set
* 和Mapreduce不一样，Spark中多个Task可以运行在一个进程中，而且这个进程的生命周期和Application一样，即使没有job运行

**优点**：Spark加快运行速度，Task可以快速启动，并处理内存中的数据

**缺点**：每个Application拥有固定数量的executor和固定数量的内存(executor就是一个进程，对应一个container)

### Spark组成
* 应用程序：由一个driver program和多个job组成
* job：由多个stage组成
* stage：对应一个taskset
* taskset：对应一组关联的相互之间没有shuffle依赖关系的task组成
* task：任务最小的单元

### Driver Program
* (驱动程序)是Spark的核心组件
* 构建SparkContext(Spark应用的入口，创建需要的变量，还包含集群的配置信息等)
* 将用户提交的job转化为DAG图(类似数据处理的流程图)
* 根据策略将DAG图划分成多个stage，根据分区从而生成一系列tasks
* 根据tasks要求向RM申请资源
* 提交任务并检测任务状态

### Executor
* 真正执行task的单元，一个工作节点上可以有多个executor

### mapreduce的问题
* 调度慢，启动map、reduce太耗时
* 计算慢，每一步都要保存结果到磁盘
* API抽象简单，只有map和reduce原语
* 缺乏作业流描述，一项任务需要多轮MR

### 什么是Spark
* 也是一个分布式并行的计算框架
* Spark是下一代mapreduce，扩展了数据流处理
* executor都是装载在container里运行，container默认内存是1G(参数yarn.scheduler.minimum-allocation-mb定义)
* executor分配的内存是executor-memory，向yarn申请的内存是(executor-memory+1)*num-executors
* AM在Spark中叫Driver，AM向RM申请的executor资源，当分配完资源后，executor启动后，由spark的AM向executor分配task，分配多少task，分配到哪个executor由AM决定，可理解为spark也有个调度过程，这些task都运行在executor的坑里
* executor由线程池多线程管理这些坑内的task

### Spark解决了什么问题？
* 最大化利用内存cache
* 中间结果放内存，加速迭代
* 某结果集放内存，加速后续查询和处理，解决运行慢的问题
* 更丰富的API(解决了API单一问题)
* Transfomation变换api，比如map可对每一行做变换，filter过滤出符合条件的行等，这些api实现用户算法灵活
* spark提供很多转换动作，很多基本操作如join，groupby已经在RDD转换和动作中的实现。不需用户自己实现
* 完整作业描述
* 将用户的整个作业串起来

### Spark核心
* Spark基于弹性分布式数据集(RDD)模型，具有良好的通用性，容错性与并行处理数据的能力
* RDD(Resilient Distributed Dataset)：弹性分布式数据集(相当于集合)，它的本质是数据集的描述(只读的、可分区的分布式数据集)，而不是数据集本身

### RDD的关键特征
* RDD使用户能够显式计算结果保存在内存中，控制数据的划分，并使用更丰富的操作集合来处理
* 使用更丰富的操作来处理，只读(由一个RDD变换得到另一个RDD，但是不能对本身的RDD修改)
* 记录数据的变换而不是数据本身保证容错(lineage)
	* 通常在不同机器上备份数据或记录数据更新的方式完成容错，但这种对任务密集型任务代价很高
	* RDD采用数据应用变换(map,filter,join)，若部分数据丢失，RDD拥有足够的信息得知这部分数据是如何计算得到的，可通过重新计算来得到丢失的数据
	* 这种恢复数据方式很快，无需大量数据复制操作，可以认为spark是基于RDD模型的系统
* 懒操作，延时计算，action的时候才操作
* 瞬时性，用时才产生，用完就释放

### Spark允许从以下四个方面构建RDD
* 从共享文件系统中获取 
	* val a = sc.textFile("/xxxx/file")
* 通过现有的RDD转换得到
	* val b = a.map((_,1))
* 定义一个scala数组
	* val c = sc.parallelize(1 to 10,1)
* 有一个已经存在的RDD通过持久化操作生成
	* val d = a.persist()
	* a.saveAsHadoopFile("/xxxx/file")

### Spark针对RDD提供两类操作：transfomation和action
* transfomation是RDD之间的变换，action会对数据执行一定的操作
* transfomation采用懒策略，仅在对相关RDD进行action提交时才触发计算

**transfomation**：map,filter,flatMap,sample,groupByKey,reduceByKey,union,join,cogroup,crossProduct,mapValues,sort,partitionBy

**action**:
count,collect,reduce,lookup,save

### 依赖关系
每个RDD包含了数据分块/分区(partition)的集合，每个partition是不可分割的

* 实际数据快的描述(实际数据到底存哪，或者不存在)
* 其值依赖于哪些partition

与父RDD的依赖关系(rddA => rddB)

* 宽依赖：A与B是一对多的关系
比如groupByKey,reduceByKey,join,由A产生B时会先对A做shuffle分桶
* 窄依赖：B的每个partition依赖于A的常数个partition(1对1)
比如map,filter,union

每个partition的计算就是一个task，task是调度的基本单位。若一个stage包含的其他stage中的任务已经全部完成，这个stage中的任务才会被加入调度
遵循数据局部性原则，使得数据传输代价最小

* 如果一个任务需要的数据在某个节点的内存中，这个任务就会被分配至那个节点
* 需要的数据在某个节点的文件系统中，就分配至那个节点

(此时的调度指的是：由spark的AM来决定计算partition的task，分配到哪个executor上)

### 容错
* 如果此task失败，AM会重新分配task
* 如果task依赖的上层partition数据已经失效，会先将其依赖的partition计算任务再重算一遍
* 宽依赖中被依赖partition，可以将数据保存HDFS，以便快速重构(checkpoint)
* 窄依赖只依赖上一层partition，恢复代价较少，宽依赖依赖上层所有的partition，如果数据丢失，上层所有partition要重算
* 可以指定保存一个RDD的数据至节点的cache中，如果内存不够，会LRU释放一部分，仍有重构的可能

### Spark作业原理
Driver Process(AM)向YARN(RM)申请资源，RM分配资源后，AM分配task，每个task下，cache和persist占用的内存为60%，spark.storage.memoryFraction=60%，如果没加cache或persist，这个内存是没有用起来的。shuffle阶段需要20%，spark.shuffle.memoryFraction=20%，其他计算需要的内存是剩下的20%，这是默认情况下，实际可以调整。

### run脚本参数调优
**num-executors:**该作业总共需要多少executor进程执行

建议：5,10,20左右比较合适

**executor-memory:**设置每个executor进程的内存，num-executors*executor-memory代表作业申请的总内存

建议：5G-10G较合适

**executor-cores:**每个executor进程的CPU核数量，该参数决定每个executor进程并行执行task线程的能力，num-executors*executor-cores代表作业申请的总核数

建议：2-4较合适

**driver-memory:**设置Driver进程的内存

-建议：通常不用设置，一般1G就够了，若出现使用collect算子将RDD数据全部拉取到Driver上处理，就必须确保该值足够大，否则OOM内存溢出

**spark.default.parallelism:**每个stage的默认task数量

建议：设置500-1000较合适，默认一个HDFS的block对应一个task，Spark默认值偏少，这样导致不能充分利用资源

**spark.storage.memoryFraction:**设置RDD持久化数据在executor内存中能占的比例，默认0.6，即默认executor60%内存可以保存持久化RDD数据

建议：若有较多的持久化操作，可以设置高些，超出内存的会频繁gc导致运行缓慢

**spark.shuffle.memoryFraction:**聚合操作占executor内存的比例，默认0.2

建议：若持久化操作较少，但shuffle较多时，可以降低持久化内存占比，提高shuffle操作内存占比


### spark开发调优
##### 原则一：避免创建重复的RDD
--对同一份数据，只应该创建一个RDD，不能创建多个RDD来代表同一份数据

--极大浪费内存

##### 原则二：尽可能复用同一个RDD
--比如：一个RDD数据格式是key-value，另一个是单独value类型，这两个RDD的value部分完全一样，这样可以复用达到减少算子执行次数

##### 原则三：对多次使用的RDD进行持久化处理
--每次对一个RDD执行一个算子操作时，都会重新从源头处理计算一遍，计算出那个RDD出来，然后进一步操作，这种方式性能很差

--对多次使用的RDD进行持久化，将RDD的数据保存在内存或磁盘中，避免重复劳动

--借助cache()和persist()方法

|持久化级别|含义解释|
|---|---|
|MEMORY_ONLY|使用未序列化的java对象格式，将数据保存在内存中，如果内存不够存放所有数据，则数据可能就不会进行持久化，那么下次对这个RDD执行算子操作时，那些没有被持久化的数据，需要从源头处重新计算一遍，这是默认的持久化策略，使用cache()方法时，实际就是使用这种持久化策略|
|MEMORY_AND_DISK|使用未序列化的java对象格式，优先尝试将数据保存在内存中，如果内存不够存放所有的数据，会将数据写入磁盘文件中，下次对这个RDD执行算子时，持久化在磁盘中的数据会被读取出来使用|
|MEMORY_ONLY_SER|基本含义同MEMORY_ONLY，唯一的区别，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组，这种方式更加省内存，从而可以避免持久化的数据占用过多内存导致频繁GC|
|MEMORY_AND_DISK_SER|基本含义同MEMORY_AND_DISK，唯一的区别，会将RDD中的数据进行序列化，RDD的每个partition会被序列化成一个字节数组，这种方式更加省内存，从而可以避免持久化的数据占用过多内存导致频繁GC|
|DISK_ONLY|使用未序列化的java对象格式，将数据全部写入磁盘文件中|
|MEMORY_ONLY_2,MEMORY_AND_DISK_2,等等|对于上述任意一种持久化策略，如果加上后缀_2，代表的是将每个持久化数据都复制一份，并将副本保存到其他节点上，这种基于副本的持久化机制主要用于进行容错，假如某个节点挂了，节点的内存或者磁盘中的持久化数据丢失了，那么后续对RDD计算时还可以使用该数据在其他节点上的副本，如果没有副本的话，就只能将这些数据从源头处重新计算一遍|

##### 原则四：避免使用shuffle类算子
--在spark作业运行过程中，最消耗性能的地方就是shuffle过程

--将分布在集群中多个节点上的同一个key，拉取到同一个节点上，进行聚合和join处理，比如groupByKey，reduceByKey，join等算子，都会触发shuffle

##### 原则五：使用map-side预聚合的shuffle操作
--一定要使用shuffle的，无法用map类算子替换的，那么尽量使用map-side预聚合的算子

--思想类似MapReduce的Combiner

--可能的情况下使用reduceByKey或aggregateByKey算子替代groupByKey算子，因为reduceByKey或aggregateByKey算子会使用用户自定义的函数对每个节点本地相同的key进行预聚合，而groupByKey算子不会预聚合

##### 原则六：使用Kryo优化序列化性能
--Kryo是一个序列化类库，来优化序列化和反序列化性能

--Spark默认使用java序列化机制(ObjectInputStream和ObjectOutputStream)进行序列化和反序列化

--Spark支持使用Kryo序列化库，性能比java序列化库高很多，10倍左右

### Spark技术栈
* Spark和Hadoop关系：Spark依赖于HDFS文件系统，如果是Spark on Yarn部署模式，又依赖于yarn计算框架
* Spark Core：基于RDD提供操作接口，利用DAG进行统一的任务规划
* Spark SQL：Hive表+Spark，通过把Hive的HQL转化为Spark DAG计算来实现
* Spark Streaming：Spark的流式计算框架
* RDD中MLlib/DataFrame中ML：Spark的机器学习库，包含常用的机器学习算法
* GraphX：Spark图并行操作库
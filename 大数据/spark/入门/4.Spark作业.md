# Spark作业

---

## 作业1
**统计每个用户有多少不同小时的购物时间**

	val df_count = df.select("user_id","order_hour_of_day").distinct().groupBy("user_id").count()
	df_count.show()

## 作业2
**对每个用户的order_number和order_hour_of_day的组合，按照order_hour_of_day排序**
	
	val df = spark.sql("select * from orders")

	df.select("user_id","order_number","order_hour_of_day").rdd.take(10)
	res1: Array[org.apache.spark.sql.Row] = Array([1,1,08], [1,2,07], [1,3,12], [1,4,07], [1,5,15], [1,6,07], [1,7,09], [1,8,14], [1,9,16], [1,10,08])

	df.select("user_id","order_number","order_hour_of_day").rdd.map(x => (x(0).toString,(x(1).toString,x(2).toString))).take(10)
	res3: Array[(String, (String, String))] = Array((1,(1,08)), (1,(2,07)), (1,(3,12)), (1,(4,07)), (1,(5,15)), (1,(6,07)), (1,(7,09)), (1,(8,14)), (1,(9,16)), (1,(10,08)))

	val map = df.select("user_id","order_number","order_hour_of_day").rdd.map(x => (x(0).toString,(x(1).toString,x(2).toString)))

## 作业3
**对order_number和order_dow求和**

	df.selectExpr("*","cast(order_number as int)+cast(order_dow as int) as order_sum").show

	select *,(cast(order_number as int)+cast(order_dow as int)) as order_sum from orders limit 10;

	val sum_udf = udf((x:String,y:String) => x.toInt+y.toInt)
	df.withColumn("order_sum",sum_udf(col("order_number"),col("order_dow"))).show()

## 作业4
**在集群中读取hive表**

	val spark = SparkSession.builder()
      .appName("spark_test")
	  .enableHiveSupport()
      .getOrCreate()
    spark.sql("show tables").show();

	--run.sh脚本
	--local可以替换为yarn-client和yarn-cluster
	cd /home/apps/spark/spark-2.0.2/
	./bin/spark-submit \
		--class com.admin.spark_test \
		--master local \
		--files /home/apps/hive/hive-1.2.2/conf/hive-site.xml \
		/opt/spark/spark-1.0-SNAPSHOT.jar

## 作业5
1.统计product被购买的数据量
2.统计product被reordered的数量(再次购买)
3.结合上面数量统计product购买的reordered的比率
	

	val prior: DataFrame = spark.sql("select * from test.order_products_prior")
    //统计product被购买的数据量
    prior.groupBy("product_id").count().as("prod_cnt")
    .take(10).foreach(println(_))
    //统计product被reordered的数量(再次购买)
    //结合上面数量统计product购买的reordered的比率
    prior.selectExpr("product_id","cast(reordered as int)")
      .groupBy("product_id")
      .agg(
        sum("reordered").as("reordered_sum"),//reordered的数量
        avg("reordered").as("reordered_rate")//reordered的比率
      ).take(10).foreach(println(_))

## 作业5
1.每个用户购买订单的平均间隔
2.每个用户的总订单数
3.每个用户购买的product商品去重后的数据
4.用户总商品数量以及去重后的商品数量
5.每个用户购买的平均每个订单商品数量

	import orders.sparkSession.implicits._
    val orders = spark.sql("select * from test.orders")
    val priors = spark.sql("select * from test.order_products_prior")

	//异常处理，把null值改成0
    val orders_new = orders
      .selectExpr("*","if(days_since_prior_order is null,0.0,days_since_prior_order) as dspo")
      .drop("days_since_prior_order")

	//1.求每个用户购买订单的平均间隔，并重命名
    val orders_period = orders_new
      .selectExpr("user_id", "cast(dspo as double)")
      .groupBy("user_id")
      .avg("dspo")
      .withColumnRenamed("avg(dspo)", "orders_period")

	//2.每个用户的总订单数
	val orders_count = orders
      .selectExpr("user_id", "cast(order_id as int)")
      .groupBy("user_id")
      .count()
      .withColumnRenamed("count", "order_cnt")

	//3.每个用户购买的product商品去重后的数据
	val products_set = orders
      .join(priors, "order_id")
      .select("user_id", "product_id")
      .rdd.map(x => (x(0).toString, x(1).toString))
      .groupByKey()
      .mapValues(arr => {
        (arr.size, arr.toSet.size, arr.toSet.mkString(","))
      }).toDF("user_id", "tupple")
      .selectExpr("user_id",
        "tupple._1 as prod_cnt",
        "tupple._2 as dist_prod_cnt",
        "tupple._3 as product_set")

	//4.用户总商品数量以及去重后的商品数量
	orders
      .join(priors,"order_id")
      .select("user_id","product_id")
      .rdd.map(x=>(x(0).toString,x(1).toString))
      .groupByKey()
      .mapValues(arr=>{
        (arr.size,arr.toSet.size,arr.toSet.mkString(","))
      }).toDF("user_id","tupple")
      .selectExpr("user_id",
        "tupple._1 as all_size",
        "tupple._2 as dist_size",
        "tupple._3 as product_set")

	//5.每个用户购买的平均每个订单商品数量
	val orders_avg_prod = priors
      .select("order_id","product_id")
      .groupBy("order_id")
      .count()
      .withColumnRenamed("count","product_cnt")
      .join(orders,"order_id")
      .groupBy("user_id")
      .avg("product_cnt")
      .withColumnRenamed("avg(product_cnt)","product_avg")

	//拼接
	val result = orders_period
      .join(orders_count,"user_id")
      .join(products_set,"user_id")
      .join(orders_avg_prod,"user_id")
      .selectExpr("cast(user_id as int)",
        "orders_period",
        "order_cnt",
        "prod_cnt",
        "dist_prod_cnt",
        "product_avg",
        "product_set")
      .orderBy("user_id")

	//其他思路
	//每个用户购买的商品集合
    orders
      .join(priors, "order_id")
      .select("user_id", "product_id")
      .rdd.map(x => (x(0).toString, x(1).toString))
      .groupByKey()
      .mapValues(_.toSet.mkString(","))
      .toDF("user_id", "product_set")

	//每个用户购买的平均每个订单商品数量
	orders
      .join(priors, "order_id")
      .select("user_id", "order_id", "product_id")
      .groupBy("user_id","order_id")
      .count()
      .withColumnRenamed("count","product_cnt")
      .groupBy("user_id")
      .avg("product_cnt")
      .withColumnRenamed("avg(product_cnt)","product_avg")

	
# Spark安装

### 一、解压

### 二、在$SPARK_HOME/conf目录下

##### 1.复制配置文件
	cp spark-env.sh.template spark-env.sh
	cp slaves.template slaves

##### 2.编辑slaves文件
	编辑slaves写入从节点slave1 slave2

##### 3.编辑spark-env.sh文件
	export JAVA_HOME=/home/apps/jdk/jdk8
	export HADOOP_HOME=/home/apps/hadoop/hadoop-2.6.1
	export SCALA_HOME=/home/apps/scala/scala-2.11.8
	export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop
	SPARK_MASTER_HOST=master
	SPARK_LOCAL_DIRS=/home/apps/spark/spark-2.0.2
	SPARK_DRIVER_MEMORY=1G

### 三、配置完成拷贝到其他节点

### 四、启动sbin/start-all.sh

### 五、spark如果需要读取hive数据
##### 1.需要将Hive中的hive-site.xml文件拷贝到Spark的conf目录下
##### 2.如果是mysql存储的元数据，需要把mysql的jar包放到jars目录下
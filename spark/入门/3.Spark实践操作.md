# Spark实践操作

---

### client模式下
	--获得数据集
	val df = spark.sql("select * from badou.orders")

	--查看数据集(默认展示20条)
	df.show()   
	df.show

	--dataframe变成rdd
	val rdd = df.select("order_dow").rdd
	rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[20] at rdd at <console>:25

	--rdd展示前10条
	rdd.take(10)
	res7: Array[org.apache.spark.sql.Row] = Array([2], [3], [3], [4], [4], [2], [1], [1], [1], [4])

	--rdd过滤(注意rdd的结构)
	rdd.filter(_(0)=="1").take(10)
	res10: Array[org.apache.spark.sql.Row] = Array([1], [1], [1], [1], [1], [1], [1], [1], [1], [1])
	
	--rdd过滤
	rdd.map(_(0).toString).filter(_=="1").take(10)
	res11: Array[String] = Array(1, 1, 1, 1, 1, 1, 1, 1, 1, 1)

### wordcount
	val order_dow=rdd.map(_(0).toString).take(10)
	res13: Array[String] = Array(2, 3, 3, 4, 4, 2, 1, 1, 1, 4)

	--统计每个order_dow的记录数
	order_dow.map((_,1)).reduceByKey(_+_).take(10)
	res15: Array[(String, Int)] = Array((4,426339), (5,453368), (6,448761), (0,600905), (2,467260), (3,436972), (1,587478))

### cache
	--加载到内存，本身count是懒加载
	val df_cache = df.groupBy("order_dow").count().cache

	--第一次执行会慢些，第二次执行会读取内存
	df_cache.show()
	
	--清除内存
	df_cache.unpersist()

	val priors = spark.sql("select * from badou.order_products_prior")
	val df_join = df.join(priors,"order_id").cache
	df_join.show()  
	--同样第一次执行很慢，第二次执行很快，join操作比较明显
	df_join.unpersist()

### groupBy和groupByKey的区别
	--groupBy需要指定参数作为分组key，分组后的value元素包含本身的key
	map.groupBy(_._1).take(10)
	res5: Array[(String, Iterable[(String, (String, String))])] = Array((1,CompactBuffer((1,(1,08)), (1,(2,07)), (1,(3,12)), (1,(4,07)), (1,(5,15)), (1,(6,07)), (1,(7,09)), (1,(8,14)), (1,(9,16)), (1,(10,08)), (1,(11,08)))), (2,CompactBuffer((2,(1,11)), (2,(2,10)), (2,(3,10)), (2,(4,10)), (2,(5,11)), (2,(6,09)), (2,(7,12)), (2,(8,15)), (2,(9,09)), (2,(10,11)), (2,(11,10)), (2,(12,09)), (2,(13,11)), (2,(14,10)), (2,(15,11)))), (3,CompactBuffer((3,(1,14)), (3,(2,19)), (3,(3,16)), (3,(4,18)), (3,(5,17)), (3,(6,16)), (3,(7,15)), (3,(8,17)), (3,(9,16)), (3,(10,16)), (3,(11,18)), (3,(12,15)), (3,(13,15)))), (4,CompactBuffer((4,(1,11)), (4,(2,11)), (4,(3,15)), (4,(4,13)), (4,(5,13)), (4,(6,12)))), (5,CompactBuffer((5,(1,12)), (5,(2,16)), (5,(3,18)), (5,(4,18)), (5,(5,11)))), (6,CompactBuffer((6,(1...

	--groupByKey默认以key分组，分组后的value元素不包含本身的key
	map.groupByKey().take(3)
	res9: Array[(String, Iterable[(String, String)])] = Array((2828,CompactBuffer((1,08), (2,17), (3,11), (4,10), (5,10), (6,16), (7,16), (8,08), (9,10), (10,10), (11,08), (12,08), (13,08), (14,11), (15,09), (16,10), (17,12), (18,10), (19,18), (20,15), (21,15), (22,09))), (41996,CompactBuffer((1,08), (2,17), (3,13), (4,15), (5,14), (6,08), (7,13), (8,15), (9,11), (10,16), (11,09), (12,16), (13,11), (14,15), (15,17), (16,14), (17,10), (18,15), (19,14), (20,12), (21,12), (22,16), (23,15), (24,16), (25,11), (26,14), (27,15), (28,12), (29,14), (30,15), (31,16), (32,15), (33,09), (34,11), (35,16), (36,14), (37,12), (38,15), (39,11), (40,17), (41,17), (42,12), (43,14), (44,10), (45,16), (46,10), (47,09), (48,17), (49,16), (50,08), (51,13), (52,08), (53,10), (54,09), (55,11), (56,08), (57,08), (58...

	map.groupByKey().mapValues(_.toList.sortBy(_._2.toInt)).take(10)
	res11: Array[(String, List[(String, String)])] = Array((2828,List((1,08), (8,08), (11,08), (12,08), (13,08), (15,09), (22,09), (4,10), (5,10), (9,10), (10,10), (16,10), (18,10), (3,11), (14,11), (17,12), (20,15), (21,15), (6,16), (7,16), (2,17), (19,18))), (41996,List((1,08), (6,08), (50,08), (52,08), (56,08), (57,08), (61,08), (83,08), (11,09), (33,09), (47,09), (54,09), (62,09), (64,09), (78,09), (90,09), (95,09), (17,10), (44,10), (46,10), (53,10), (59,10), (60,10), (68,10), (79,10), (87,10), (9,11), (13,11), (25,11), (34,11), (39,11), (55,11), (66,11), (73,11), (20,12), (21,12), (28,12), (37,12), (42,12), (58,12), (70,12), (3,13), (7,13), (51,13), (75,13), (86,13), (89,13), (91,13), (92,13), (94,13), (97,13), (98,13), (99,13), (100,13), (5,14), (16,14), (19,14), (26,14), (29,14), (3...

### 数据转换成dataframe

	import spark.implicits._
	val result = map.groupByKey().mapValues(_.toList.sortBy(_._2.toInt)).toDF("user_id","values")
	result.cache
	result.show(false)
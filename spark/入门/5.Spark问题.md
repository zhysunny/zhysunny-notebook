
1.cluster模式运行spark发现找不到hive表，那么启动脚本中加上

	--files hive-site.xml全路径

2.client模式错误：

	Exception in thread "main" org.apache.spark.SparkException: Yarn application has already ended! It might have been killed or unable to launch application master.

	Diagnostics: Container [pid=44397,containerID=container_1553316317500_0101_02_000001] is running beyond virtual memory limits. Current usage: 103.3 MB of 1 GB physical memory used; 2.2 GB of 2.1 GB virtual memory used. Killing container.

	解决办法
	在yarn-site.xml中增加配置值，然后重启hadoop
	<!-- 虚拟内存设置是否生效 -->
	<property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>
	或者加这个配置，两个都加也可以，下面这个配置没试过
	<!-- 虚拟内存/物理内存的值，默认2.1 -->
	<property>
        <name>yarn.nodemanager.vmem-pmem-ratio</name>
        <value>4</value>
    </property>